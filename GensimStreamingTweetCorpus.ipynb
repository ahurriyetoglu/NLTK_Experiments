{
 "metadata": {
  "name": "GensimStreamingTweetCorpus"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "This code analyzes corpus of tweets using Gensim"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "plt.rc('figure', figsize=(8, 5))\n",
      "sys.path.append('/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/')\n",
      "sys.path.append('/Users/doug/SW_Dev/NLTK_Experiments')\n",
      "import nltk\n",
      "import pprint\n",
      "import json\n",
      "import os\n",
      "import gzip\n",
      "import json\n",
      "import collections\n",
      "import operator\n",
      "from BeautifulSoup import BeautifulSoup, NavigableString\n",
      "from ttp import ttp\n",
      "import string\n",
      "import logging\n",
      "from gensim import corpora, models, similarities\n",
      "import logging, gensim, bz2\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Welcome to pylab, a matplotlib-based Python environment [backend: module://IPython.zmq.pylab.backend_inline].\n",
        "For more information, type 'help(pylab)'.\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dir_path_root = '/Users/doug/SW_Dev/NLTK_Experiments/MyCorpus/'\n",
      "dir_path = dir_path_root+'Samples-09112013evening/'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#generator function to return all tweets in a zipped firectory\n",
      "def all_tweets(directory, extract=lambda x:json.loads(x)):\n",
      "    for fn in os.listdir(directory):\n",
      "        if fn.endswith('.json.gz'):\n",
      "            f = gzip.open(directory+fn, 'rb')\n",
      "            for line in f:\n",
      "                yield extract(line)\n",
      "            f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def myfilter(x):\n",
      "    x = json.loads(x)\n",
      "    \n",
      "    #pprint.pprint(x)\n",
      "    #raise Exception\n",
      "    result={}\n",
      "    if 'delete' not in x:\n",
      "        result['id_str'] = x['id_str']\n",
      "        #result['text'] = x['text']\n",
      "        result['screen_name'] = x['user']['screen_name']\n",
      "        #result['name'] = x['user']['name']\n",
      "        result['user_mentions'] = [men['screen_name'].lower() for men in x['entities']['user_mentions']]\n",
      "        result['hashtags'] = [entry['text'].lower() for entry in x['entities']['hashtags']]\n",
      "        result['lang'] = x['lang']\n",
      "        return result\n",
      "    else:\n",
      "        False"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#test routine for all_tweets\n",
      "total_tweets = 0\n",
      "user_dict = corpora.Dictionary([])\n",
      "ment_dict = corpora.Dictionary([])\n",
      "hash_dict = corpora.Dictionary([])\n",
      "def foo():\n",
      "    for x in all_tweets(dir_path, extract=myfilter):\n",
      "        if x and x['lang']=='en':\n",
      "            total_tweets += 1\n",
      "            user_dict.doc2bow([x['screen_name']], allow_update=True)\n",
      "            hash_dict.doc2bow(x['hashtags'], allow_update=True)\n",
      "            ment_dict.doc2bow(x['user_mentions'], allow_update=True)\n",
      "\n",
      "    print 'total english tweets: ', total_tweets\n",
      "\n",
      "#foo()\n",
      "#user_dict.filter_extremes(no_below=5, no_above=0.2, keep_n=1000)\n",
      "#ment_dict.filter_extremes(no_below=5, no_above=0.2, keep_n=50)\n",
      "#hash_dict.filter_extremes(no_below=5, no_above=0.2, keep_n=50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Attempt at creating my own corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def parse_tweet(text, include_text=True, include_users=False, include_tags=False, include_reply=False, include_url=False):\n",
      "    \"Simple Tweet Parser\"\n",
      "    result = []\n",
      "    tweet = ttp.Parser().parse(text)\n",
      "    \n",
      "    if include_text:\n",
      "        soup = BeautifulSoup(tweet.html)\n",
      "        result =  [x.split() for x in soup.contents if type(x) is NavigableString]\n",
      "        result = [item for sublist in result for item in sublist]  #flatten list\n",
      "        result = [''.join(c for c in s if c not in string.punctuation) for s in result] # remove punctuation\n",
      "        result = [s for s in result if s] # empty empty strings\n",
      "        \n",
      "    if include_reply:\n",
      "        result.append(\"@{}\".format(tweet.reply))\n",
      "        \n",
      "    if include_users:\n",
      "        result = result + [\"@{}\".format(tag) for tag in tweet.users]\n",
      "        \n",
      "    if include_tags:\n",
      "        result = result + [\"#{}\".format(tag) for tag in tweet.tags]\n",
      "        \n",
      "    if include_url:\n",
      "        result = result + tweet.urls\n",
      "    \n",
      "    return result\n",
      "\n",
      "#parse_tweet(\"@ianozsvald, you now support #IvoWertzel's tweet parser! https://github.com/ianozsvald/\", include_reply=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#logging.basicConfig(level=logging.WARNING)\n",
      "\n",
      "def default_extract(line):\n",
      "        tweet = json.loads(line)\n",
      "        #pprint.pprint(tweet)\n",
      "        if ('delete' not in tweet) and ('disconnect' not in tweet):\n",
      "            return parse_tweet(tweet['text'])\n",
      "        else:\n",
      "            False\n",
      "\n",
      "class MyCorpus(corpora.TextCorpus):\n",
      "    stoplist = set('for a of the and to in on'.split())\n",
      "    \n",
      "    def __init__(self, corpus_prefix=None, corpus_path_root=None, count=None, sample=None, extract=None):\n",
      "        self.input = input\n",
      "        self.dict = corpora.Dictionary()\n",
      "        self.stop_words = nltk.corpus.stopwords.words('english')\n",
      "        if corpus_path_root:\n",
      "            self.corpus_path = corpus_path_root\n",
      "        else:\n",
      "            self.corpus_path = '/Users/doug/SW_Dev/NLTK_Experiments/MyCorpus/'\n",
      "            \n",
      "        if corpus_prefix:\n",
      "            self.corpus_path = self.corpus_path + corpus_prefix + '/'\n",
      "        else:\n",
      "            self.corpus_path = self.corpus_path + 'UCLA/'\n",
      "            \n",
      "        if extract:\n",
      "            self.extract = extract\n",
      "        else:\n",
      "            self.extract = default_extract\n",
      "\n",
      "\n",
      "    def get_texts(self, limit=False):\n",
      "        count = 0\n",
      "        \n",
      "        for fn in os.listdir(self.corpus_path):\n",
      "            if fn.endswith('.json.gz'):\n",
      "                f = gzip.open(self.corpus_path+fn, 'rb')\n",
      "                try:\n",
      "                    for line in f:\n",
      "                        if limit and count >= limit:\n",
      "                            raise Exception('limit')\n",
      "                        tokens = set(self.extract(line))\n",
      "                        tokens = tokens.difference(self.stop_words)\n",
      "                        if tokens:\n",
      "                            yield tokens\n",
      "                            count += 1\n",
      "                except IOError:\n",
      "                    pass\n",
      "                except Exception:\n",
      "                    pass\n",
      "                f.close()\n",
      "\n",
      "    def __iter__(self, limit=False, allow_update=True):\n",
      "        for tokens in self.get_texts(limit=limit):\n",
      "            #d = self.dict\n",
      "            yield(self.dict.doc2bow(tokens, allow_update=allow_update))\n",
      "\n",
      "    def get_dict(self):\n",
      "        return self.dict\n",
      "\n",
      "    def __len__(self):\n",
      "        \"\"\"Define this so we can use `len(corpus)`\"\"\"\n",
      "        if 'length' not in self.__dict__:\n",
      "            #logger.info(\"caching corpus size (calculating number of documents)\")\n",
      "            #self.length = sum(1 for doc in self.get_texts())\n",
      "            self.length = sum(1 for doc in self)\n",
      "        return self.length"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m = MyCorpus()\n",
      "print 'Corpus size: ', len(m)\n",
      "print m.get_dict()\n",
      "id2word = m.get_dict()\n",
      "\n",
      "# extract 100 LDA topics, using 1 pass and updating once every 1 chunk (10,000 documents)\n",
      "#lda = gensim.models.ldamodel.LdaModel(corpus=m, id2word=id2word, num_topics=100, update_every=1, chunksize=10000, passes=20)\n",
      "lda = gensim.models.ldamodel.LdaModel(corpus=m, id2word=id2word, num_topics=100, update_every=0, passes=20)\n",
      "lda.print_topics(20)\n",
      "print 'done'\n",
      "lda.show_topics()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Corpus size:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1084\n",
        "Dictionary(2420 unique tokens)\n",
        "done"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "[u'0.041*tomorrow + 0.028*vs + 0.026*RT + 0.018*see + 0.018*upset + 0.018*Unveils + 0.018*Team + 0.018*Stream + 0.018*Late + 0.018*Image',\n",
        " u'0.065*125 + 0.065*175 + 0.065*360 + 0.065*99 + 0.065*Season + 0.065*Terrible + 0.065*Tix + 0.065*prices + 0.065*seats + 0.065*Student',\n",
        " u'0.062*freshman + 0.062*linemen + 0.062*learning + 0.062*job + 0.041*Nose + 0.041*tackle + 0.041*Kenny + 0.041*Clark + 0.021*All + 0.021*yesterday',\n",
        " u'0.047*win + 0.045*Week + 0.045*3 + 0.036*Barr + 0.036*Defensive + 0.036*Player + 0.036*11 + 0.036*Anthony + 0.036*National + 0.036*LB',\n",
        " u'0.074*Nebraska + 0.070*way + 0.063*Athletics + 0.063*great + 0.063*181 + 0.063*supporters + 0.051*RT + 0.028*official + 0.028*Its + 0.021*3',\n",
        " u'0.075*RT + 0.054*fact + 0.054*won + 0.054*fans + 0.054*headtohead + 0.054*sight + 0.054*lose + 0.054*Groce + 0.054*John + 0.054*dont',\n",
        " u'0.056*Donahue + 0.056*Terry + 0.053*coach + 0.050*RT + 0.050*scores + 0.050*28pts + 0.050*100th + 0.050*1988 + 0.050*1stQ + 0.050*4128',\n",
        " u'0.047*The + 0.041*one + 0.038*RT + 0.029*httpt\\u2026 + 0.029*many + 0.029*weekend + 0.029*games + 0.029*big + 0.029*closer + 0.029*look',\n",
        " u'0.038*RT + 0.038*Fair + 0.038*year + 0.038*40000 + 0.038*Activities + 0.038*allocated + 0.038*fund + 0.038*Enormous + 0.038*Bruin + 0.038*Bash',\n",
        " u'0.050*college + 0.030*I + 0.030*year + 0.030*thought + 0.030*black + 0.030*uniform + 0.030*addition + 0.030*doubt + 0.030*shameful + 0.030*As']"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lda.print_topics(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "#import my_feature_ex as fx\n",
      "import my_word_cloud as wcloud"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sorted_counts(wc):\n",
      "    sorted_wc = sorted(wc.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
      "    sorted_wc = [val for val in sorted_wc if val[1]>1]   #prune singleton values\n",
      "    #print len(sorted_wc)\n",
      "    words = [word for word, count in sorted_wc]\n",
      "    counts = [count for word, count in sorted_wc]\n",
      "    return words, counts\n",
      "    \n",
      "def display_cloud(wc):\n",
      "    words, counts = sorted_counts(wc)\n",
      "    print words[:20]\n",
      "    words2 =  np.array(words[:200], np.dtype('string'))\n",
      "    counts2 =  np.array(counts[:200], np.int32)\n",
      "    wcloud.display_wordcloud(words2,counts2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#display_cloud(users)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#display_cloud(mentions)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#display_cloud(hashtags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    }
   ],
   "metadata": {}
  }
 ]
}