{
 "metadata": {
  "name": "Word Clusters-PatternNLP"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "This is a refactoring of earlier Word Clusters, code moved to:\n",
      "    my_feature_ex.py\n",
      "    my_word_cloud.py"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "plt.rc('figure', figsize=(8, 8))\n",
      "\n",
      "import cPickle as pickle\n",
      "\n",
      "import sys\n",
      "sys.path.append('/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/')\n",
      "sys.path.append('/Users/doug/SW_Dev/NLTK_Experiments/')\n",
      "\n",
      "import pprint\n",
      "import collections\n",
      "import operator\n",
      "\n",
      "import my_feature_ex as fx\n",
      "import my_word_cloud as wcloud\n",
      "\n",
      "import nltk\n",
      "import json\n",
      "import re"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Welcome to pylab, a matplotlib-based Python environment [backend: module://IPython.zmq.pylab.backend_inline].\n",
        "For more information, type 'help(pylab)'.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "skipped:  111010100010\t\t212\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_tweet_corpus(fname, limit=False):\n",
      "    tweets = dict()\n",
      "    fd = open(fname)\n",
      "    i = 0\n",
      "    for json_text in fd:\n",
      "        tweet = json.loads(json_text)['tweet']\n",
      "        tweets[tweet['id_str']] = tweet\n",
      "        i += 1\n",
      "        if limit and (i >= limit):\n",
      "            break\n",
      "    print 'loaded'\n",
      "    return tweets\n",
      "\n",
      "def load_UCLA_tweet_corpus(limit=False):\n",
      "    fname = '/Users/doug/Desktop/NLP/sentiment/UCLA.json'\n",
      "    return load_tweet_corpus(fname, limit=limit)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Tag Format: http://www.clips.ua.ac.be/pages/MBSP-tags\n",
      "import pattern\n",
      "import pattern.text\n",
      "import pattern.en\n",
      "#from pattern.en import parsetree\n",
      "#sys.path.append('/Users/doug/SW_Dev/MBSP/')\n",
      "sys.path.append('/Users/doug/SW_Dev/')\n",
      "import MBSP as mbsp\n",
      "\n",
      "def mbsp_parsetree(s, *args, **kwargs):\n",
      "    \"\"\" Returns a parsed Text from the given string. \"\"\"\n",
      "    return mbsp.split(mbsp.parse(s, *args, **kwargs))\n",
      "\n",
      "def xmbsp_parsetree(s, *args, **kwargs):\n",
      "    \"\"\" Returns a parsed Text from the given string. \"\"\"\n",
      "    return pattern.text.Text(mbsp.parse(s, *args, **kwargs))\n",
      "\n",
      "RE_HASHTAG2 = re.compile(r\"(\\s*#\\w+\\s*)+$\")\n",
      "def strip_hashtags(text, hashtags):\n",
      "    text = text.encode('ascii', 'ignore')\n",
      "    m = RE_HASHTAG2.search(text)        #strip reply userid\n",
      "    if m:\n",
      "        text = text[:m.start()]\n",
      "    return text\n",
      "\n",
      "def xstrip_hashtags(text, hashtags):\n",
      "    if not hashtags:\n",
      "        return text\n",
      "    tags = [\"#{}\".format(ht['text']) for ht in hashtags]\n",
      "    tags.sort(key=lambda x: len(x),reverse=True)\n",
      "    for tag in tags:\n",
      "        text = text.replace(tag, \"\")\n",
      "    #print \"h: \", text\n",
      "    return text\n",
      "\n",
      "def strip_url(text, urls):\n",
      "    if not urls:\n",
      "        return text\n",
      "    for url in urls:\n",
      "        text = text.replace(url['url'], \"\")\n",
      "    #print \"u: \",text\n",
      "    return text\n",
      "\n",
      "#RE_REPLY = re.compile(r\"^(@\\w+\\s.)+:.\")\n",
      "RE_REPLY = re.compile(r\"^(@\\w+\\s?)+:?\\s?\")\n",
      "RE_HASHTAG = re.compile(r\"(\\s*#\\w+\\s*)+$\")\n",
      "def annotate_pos_platform(tweets, mbsp=False):\n",
      "    if mbsp:\n",
      "        my_parsetree = mbsp_parsetree\n",
      "    else:\n",
      "        my_parsetree = pattern.en.parsetree\n",
      "\n",
      "    for key in tweets.keys():\n",
      "        text = tweets[key]['text']\n",
      "        text = text.encode('ascii', 'ignore')\n",
      "        m = RE_REPLY.match(text)        #strip reply userid\n",
      "        if m:\n",
      "            text = text[m.end():]\n",
      "        #print text\n",
      "        text = text.replace('\"', \"\")    #remove any quotes\n",
      "        text = text.replace(\"@\", \"\")    #convert uservanes into nouns\n",
      "        text = text.replace(\"&amp;\", \"&\")\n",
      "        text = strip_url(text, tweets[key]['entities']['urls'])\n",
      "        #text = strip_hashtags(text, tweets[key]['entities']['hashtags'])\n",
      "        m = RE_HASHTAG.search(text)        #strip trailing hashtags\n",
      "        if m:\n",
      "            text = text[:m.start()]\n",
      "        text = text.replace('#', \"\")       #convert any remaining hashtags to text\n",
      "        #TODO: strip out hashtags, urls ...\n",
      "        #print text\n",
      "        tweets[key]['sents'] = my_parsetree(text) \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 117
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sys.path.append('/Users/doug/SW_Dev/ark-tweet-nlp-0.3.2')\n",
      "import CMUTweetTagger\n",
      "\n",
      "RUN_TAGGER_CMD = \"java -XX:ParallelGCThreads=2 -Xmx500m -jar /Users/doug/SW_Dev/ark-tweet-nlp-0.3.2/ark-tweet-nlp-0.3.2.jar\"\n",
      "RUN_TAGGER_CMD_PTB = \"java -XX:ParallelGCThreads=2 -Xmx500m -jar /Users/doug/SW_Dev/ark-tweet-nlp-0.3.2/ark-tweet-nlp-0.3.2.jar --model /Users/doug/SW_Dev/ark-tweet-nlp-0.3.2/model.ritter_ptb_alldata_fixed.20130723.txt\"\n",
      "print CMUTweetTagger.runtagger_parse(['example tweet 1', 'example tweet 2'], run_tagger_cmd=RUN_TAGGER_CMD)\n",
      "print CMUTweetTagger.runtagger_parse(['example tweet 1', 'example tweet 2'], run_tagger_cmd=RUN_TAGGER_CMD_PTB)\n",
      "\n",
      "def annotate_pos_cmu(tweets, ptb=False):\n",
      "    if ptb:\n",
      "        tagger_cmd = RUN_TAGGER_CMD_PTB\n",
      "    else:\n",
      "        tagger_cmd = RUN_TAGGER_CMD\n",
      "    ids = []\n",
      "    texts = []\n",
      "    for key, value in tweets.items():\n",
      "        ids.append(key)\n",
      "        texts.append(json.dumps({'text':value['text']}))\n",
      "    pos = CMUTweetTagger.runtagger_parse(texts, run_tagger_cmd=tagger_cmd)\n",
      "    \n",
      "    if len(ids) != len(pos):\n",
      "        raise Exception(\"Error: Tweet Tagger returned incorrect results\") \n",
      "\n",
      "    for i in range(0, len(ids)):\n",
      "        tweets[ids[i]]['pos'] = pos[i]\n",
      "        tweets[ids[i]]['tokens'] = [tag[0] for tag in pos[i]]\n",
      "\n",
      "    #print pos[0]\n",
      "    #print [tag[0] for tag in pos[0]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[('example', 'N', 0.979), ('tweet', 'V', 0.7763), ('1', '$', 0.9916)], [('example', 'N', 0.979), ('tweet', 'V', 0.7713), ('2', '$', 0.5832)]]\n",
        "[[('example', 'NN', 0.5422), ('tweet', 'NN', 0.6582), ('1', 'CD', 0.8683)], [('example', 'NN', 0.5422), ('tweet', 'NN', 0.6071), ('2', 'TO', 0.3739)]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_word_counts(tweet_set):\n",
      "    wc = collections.defaultdict(int)\n",
      "    for tweet in tweet_set:\n",
      "        for tok in tweet['tokens']:\n",
      "            wc[tok] += 1\n",
      "    sorted_wc = sorted(wc.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
      "    sorted_wc = [val for val in sorted_wc if val[1]>1]   #prune singleton values\n",
      "    #print len(sorted_wc)\n",
      "    words = [word for word, count in sorted_wc]\n",
      "    counts = [count for word, count in sorted_wc]\n",
      "    return words, counts\n",
      "    \n",
      "def display_tweet_wordcloud(tweet_set):\n",
      "    words, counts = compute_word_counts(tweet_set)\n",
      "    words2 =  np.array(words[:200], np.dtype('string'))\n",
      "    counts2 =  np.array(counts[:200], np.int32)\n",
      "    wcloud.display_wordcloud(words2,counts2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "UCLA_tweets = load_UCLA_tweet_corpus(limit=10) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loaded\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "annotate_pos_cmu(UCLA_tweets, ptb=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#subset_tweets = [t for t in UCLA_tweets.values()[:10]]\n",
      "#pprint.pprint(subset_tweets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "annotate_pos_platform(UCLA_tweets, mbsp=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 118
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#for t in UCLA_tweets.values()[:10]:\n",
      "#    pprint.pprint(t['entities'])\n",
      "#    print t['text']\n",
      "#    \n",
      "#len(UCLA_tweets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 119
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subset_tweets = [t for t in UCLA_tweets.values()[:10]]\n",
      "for t in subset_tweets:\n",
      "    print '------------'\n",
      "    print t['text']\n",
      "    for s in t['sents']:\n",
      "        print s.string\n",
      "        print 'sentiment: ', pattern.en.sentiment(s)\n",
      "        print \"subjects: \", s.subjects\n",
      "        print \"objects: \", s.objects\n",
      "        print \"verbs: \", s.verbs\n",
      "        print \"relations: \", s.relations\n",
      "        print \"pnp: \", s.pnp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "------------\n",
        "39 years ago today, Blazers selected UCLA\u2019s @BillWalton with the No. 1 overall pick in the 1974 NBA draft.\n",
        "39 years ago today , Blazers selected UCLAs BillWalton with the No. 1 overall pick in the 1974 NBA draft .\n",
        "sentiment:  (0.0, 0.0)\n",
        "subjects:  [Chunk('Blazers/NP-SBJ-1')]\n",
        "objects:  [Chunk('UCLAs BillWalton/NP-OBJ-1')]\n",
        "verbs:  [Chunk('selected/VP-1')]\n",
        "relations:  {'SBJ': {1: Chunk('Blazers/NP-SBJ-1')}, 'VP': {1: Chunk('selected/VP-1')}, 'OBJ': {1: Chunk('UCLAs BillWalton/NP-OBJ-1')}}\n",
        "pnp:  [Chunk('with the No. 1 overall pick/PNP'), Chunk('in the 1974 NBA draft/PNP')]\n",
        "------------\n",
        "Nike Unveils New University of California Logo and Uniforms http://t.co/cq4ZCa8kdh it looks like an angry #ucla bear. No cuddly bear?\n",
        "Nike Unveils New University of California Logo and Uniforms it looks like an angry ucla bear .\n",
        "sentiment:  (-0.5, 1.0)\n",
        "subjects:  [Chunk('Nike Unveils New University/NP-SBJ-1')]\n",
        "objects:  []\n",
        "verbs:  [Chunk('looks/VP-1')]\n",
        "relations:  {'SBJ': {1: Chunk('Nike Unveils New University/NP-SBJ-1')}, 'VP': {1: Chunk('looks/VP-1')}, 'OBJ': {}}\n",
        "pnp:  [Chunk('of California Logo/PNP'), Chunk('Uniforms it/PNP'), Chunk('like an angry ucla bear/PNP')]\n",
        "No cuddly bear ?\n",
        "sentiment:  (0.0, 0.0)\n",
        "subjects:  []\n",
        "objects:  []\n",
        "verbs:  []\n",
        "relations:  {'SBJ': {}, 'VP': {}, 'OBJ': {}}\n",
        "pnp:  []\n",
        "------------\n",
        "In my break from work tired AF bc I've been working since 545am #fml #work #ucla #workstudy\u2026 http://t.co/Q7G98PZrbt\n",
        "In my break from work tired AF bc I 've been working since 545am\n",
        "sentiment:  (-0.4, 0.7)\n",
        "subjects:  [Chunk('I/NP-SBJ-1')]\n",
        "objects:  []\n",
        "verbs:  [Chunk(''ve been working/VP-1')]\n",
        "relations:  {'SBJ': {1: Chunk('I/NP-SBJ-1')}, 'VP': {1: Chunk(''ve been working/VP-1')}, 'OBJ': {}}\n",
        "pnp:  [Chunk('In my break/PNP'), Chunk('from work/PNP'), Chunk('since 545am/PNP')]\n",
        "------------\n",
        "Which biz class should i sit in on? #ucla\n",
        "Which biz class should i sit in on ?\n",
        "sentiment:  (0.0, 0.0)\n",
        "subjects:  [Chunk('Which/NP-SBJ-1')]\n",
        "objects:  [Chunk('class/NP-OBJ-1')]\n",
        "verbs:  [Chunk('biz/VP-1'), Chunk('should/VP-2'), Chunk('sit/VP-3')]\n",
        "relations:  {'SBJ': {1: Chunk('Which/NP-SBJ-1')}, 'VP': {1: Chunk('biz/VP-1'), 2: Chunk('should/VP-2'), 3: Chunk('sit/VP-3')}, 'OBJ': {1: Chunk('class/NP-OBJ-1')}}\n",
        "pnp:  []\n",
        "------------\n",
        "Whoa. \"Doctors At UCLA Perform The First Live-Vined Surgery\" http://t.co/xwO5PgZ8ER via @alltwtr\n",
        "Whoa .\n",
        "sentiment:  (0.0, 0.0)\n",
        "subjects:  []\n",
        "objects:  []\n",
        "verbs:  []\n",
        "relations:  {'SBJ': {}, 'VP': {}, 'OBJ': {}}\n",
        "pnp:  []\n",
        "Doctors At UCLA Perform The First Live-Vined Surgery via alltwtr\n",
        "sentiment:  (0.0, 0.0)\n",
        "subjects:  []\n",
        "objects:  []\n",
        "verbs:  []\n",
        "relations:  {'SBJ': {}, 'VP': {}, 'OBJ': {}}\n",
        "pnp:  [Chunk('At UCLA Perform/PNP'), Chunk('via alltwtr/PNP')]\n",
        "------------\n",
        "Ethan Telfair, former Oklahoma commit Jacob Hammond &amp; Cameron Walker visited UCLA on Monday.\n",
        "Ethan Telfair , former Oklahoma commit Jacob Hammond & Cameron Walker visited UCLA on Monday .\n",
        "sentiment:  (0.0, 0.0)\n",
        "subjects:  [Chunk('former Oklahoma/NP-SBJ-1'), Chunk('Jacob Hammond & Cameron Walker/NP-SBJ-2')]\n",
        "objects:  [Chunk('UCLA/NP-OBJ-2')]\n",
        "verbs:  [Chunk('commit/VP-1'), Chunk('visited/VP-2')]\n",
        "relations:  {'SBJ': {1: Chunk('former Oklahoma/NP-SBJ-1'), 2: Chunk('Jacob Hammond & Cameron Walker/NP-SBJ-2')}, 'VP': {1: Chunk('commit/VP-1'), 2: Chunk('visited/VP-2')}, 'OBJ': {2: Chunk('UCLA/NP-OBJ-2')}}\n",
        "pnp:  [Chunk('on Monday/PNP')]\n",
        "------------\n",
        "@ABurns85 did u ever watch Westbrook at UCLA? Same exact concerns with him as well. Non stop work ethic gives me confidence he'll develop\n",
        "did u ever watch Westbrook at UCLA ?\n",
        "sentiment:  (0.0, 0.0)\n",
        "subjects:  []\n",
        "objects:  [Chunk('Westbrook/NP-OBJ-1')]\n",
        "verbs:  [Chunk('watch/VP-1')]\n",
        "relations:  {'SBJ': {}, 'VP': {1: Chunk('watch/VP-1')}, 'OBJ': {1: Chunk('Westbrook/NP-OBJ-1')}}\n",
        "pnp:  [Chunk('at UCLA/PNP')]\n",
        "Same exact concerns with him as well .\n",
        "sentiment:  (0.125, 0.1875)\n",
        "subjects:  []\n",
        "objects:  []\n",
        "verbs:  []\n",
        "relations:  {'SBJ': {}, 'VP': {}, 'OBJ': {}}\n",
        "pnp:  [Chunk('with him/PNP')]\n",
        "Non stop work ethic gives me confidence he 'll develop\n",
        "sentiment:  (0.0, 0.0)\n",
        "subjects:  [Chunk('Non stop work ethic/NP-SBJ-1'), Chunk('he/NP-SBJ-2')]\n",
        "objects:  [Chunk('me confidence/NP-OBJ-1')]\n",
        "verbs:  [Chunk('gives/VP-1'), Chunk(''ll develop/VP-2')]\n",
        "relations:  {'SBJ': {1: Chunk('Non stop work ethic/NP-SBJ-1'), 2: Chunk('he/NP-SBJ-2')}, 'VP': {1: Chunk('gives/VP-1'), 2: Chunk(''ll develop/VP-2')}, 'OBJ': {1: Chunk('me confidence/NP-OBJ-1')}}\n",
        "pnp:  []\n",
        "------------\n",
        "@ESPNNBA: 39 years ago today, Blazers selected UCLA's @BillWalton with the No. 1 overall pick in the 1974 NBA draft.\n",
        "\n",
        "http://t.co/DWHdNN6j2B\n",
        "39 years ago today , Blazers selected UCLA 's BillWalton with the No. 1 overall pick in the 1974 NBA draft .\n",
        "sentiment:  (0.0, 0.0)\n",
        "subjects:  [Chunk('Blazers/NP-SBJ-1')]\n",
        "objects:  [Chunk('UCLA/NP-OBJ-1')]\n",
        "verbs:  [Chunk('selected/VP-1')]\n",
        "relations:  {'SBJ': {1: Chunk('Blazers/NP-SBJ-1')}, 'VP': {1: Chunk('selected/VP-1')}, 'OBJ': {1: Chunk('UCLA/NP-OBJ-1')}}\n",
        "pnp:  [Chunk('with the No. 1 overall pick/PNP'), Chunk('in the 1974 NBA draft/PNP')]\n",
        "------------\n",
        "Understand STAN, ORE &amp; USC run the Pac12, recruiting wise - Much like UM, Ohio &amp; PSU - But have to tip hat to Jim Mora. UCLA sleeping giant.\n",
        "Understand STAN , ORE & USC run the Pac12, recruiting wise - Much like UM , Ohio & PSU - But have to tip hat to Jim Mora .\n",
        "sentiment:  (0.7, 0.9)\n",
        "subjects:  [Chunk('Understand STAN , ORE & USC/NP-SBJ-1')]\n",
        "objects:  [Chunk('the Pac12, recruiting/NP-OBJ-1'), Chunk('hat/NP-OBJ-2')]\n",
        "verbs:  [Chunk('run/VP-1'), Chunk('have to tip/VP-2')]\n",
        "relations:  {'SBJ': {1: Chunk('Understand STAN , ORE & USC/NP-SBJ-1')}, 'VP': {1: Chunk('run/VP-1'), 2: Chunk('have to tip/VP-2')}, 'OBJ': {1: Chunk('the Pac12, recruiting/NP-OBJ-1'), 2: Chunk('hat/NP-OBJ-2')}}\n",
        "pnp:  [Chunk('like UM , Ohio & PSU/PNP'), Chunk('to Jim Mora/PNP')]\n",
        "UCLA sleeping giant .\n",
        "sentiment:  (0.0, 0.0)\n",
        "subjects:  []\n",
        "objects:  []\n",
        "verbs:  []\n",
        "relations:  {'SBJ': {}, 'VP': {}, 'OBJ': {}}\n",
        "pnp:  []\n",
        "------------\n",
        "3 #UrbanPlanning students were awarded Eisenhower Fellowships for their work in transportation. Congrats! http://t.co/Boltmis5db\n",
        "3 UrbanPlanning students were awarded Eisenhower Fellowships for their work in transportation .\n",
        "sentiment:  (0.0, 0.0)\n",
        "subjects:  [Chunk('3 UrbanPlanning students/NP-SBJ-1')]\n",
        "objects:  [Chunk('Eisenhower Fellowships/NP-OBJ-1')]\n",
        "verbs:  [Chunk('were awarded/VP-1')]\n",
        "relations:  {'SBJ': {1: Chunk('3 UrbanPlanning students/NP-SBJ-1')}, 'VP': {1: Chunk('were awarded/VP-1')}, 'OBJ': {1: Chunk('Eisenhower Fellowships/NP-OBJ-1')}}\n",
        "pnp:  [Chunk('for their work/PNP'), Chunk('in transportation/PNP')]\n",
        "Congrats !\n",
        "sentiment:  (0.0, 0.0)\n",
        "subjects:  []\n",
        "objects:  []\n",
        "verbs:  []\n",
        "relations:  {'SBJ': {}, 'VP': {}, 'OBJ': {}}\n",
        "pnp:  []\n"
       ]
      }
     ],
     "prompt_number": 120
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "MAX_TWEETS = 100000   #subset Corpus for now to improve Clustering run time\n",
      "tweet_set = [{'text':t['text'], 'pos':t['pos'], 'raw_tokens':t['tokens']} for t in UCLA_tweets.values()[0:MAX_TWEETS]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fx.reset_features(tweet_set)\n",
      "#show_sentiment_bigrams_ptb(tweet_set[:20])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fx.reset_features(tweet_set)\n",
      "#extract_sentiment_bigrams(tweet_set)\n",
      "#display_tweet_wordcloud(tweet_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}