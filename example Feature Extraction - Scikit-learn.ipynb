{
 "metadata": {
  "name": "example Feature Extraction - Scikit-learn"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "From: http://blog.mafr.de/2012/04/15/scikit-learn-feature-extractio/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.path.append('/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/')\n",
      "import feedparser"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feed = feedparser.parse(\"http://blog.mafr.de/feed/\")\n",
      "tags = [[t.term for t in e.tags] for e in feed.entries]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print tags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[u'misc', u'documentation', u'scripting', u'web'], [u'linux', u'networking', u'web'], [u'linux', u'ubuntu'], [u'shell', u'linux', u'quick tips'], [u'linux', u'networking', u'security'], [u'tools', u'linux', u'productivity', u'rcs'], [u'shell', u'scripting'], [u'python', u'tools'], [u'linux', u'opinion', u'ubuntu'], [u'misc', u'opinion']]\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tags = [\n",
      "  \"python, tools\",\n",
      "  \"linux, tools, ubuntu\",\n",
      "  \"distributed systems, linux, networking, tools\",\n",
      "]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "REGEX = re.compile(r\",\\s*\")\n",
      "def tokenize(text):\n",
      "    return [tok.strip().lower() for tok in REGEX.split(text)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokenize(\"foo Bar, baz\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "['foo bar', 'baz']"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "vec = CountVectorizer(tokenizer=tokenize)\n",
      "data = vec.fit_transform(tags).toarray()\n",
      "print data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[0 0 0 1 1 0]\n",
        " [0 1 0 0 1 1]\n",
        " [1 1 1 0 1 0]]\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocab = vec.get_feature_names()\n",
      "vocab"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "[u'distributed systems',\n",
        " u'linux',\n",
        " u'networking',\n",
        " u'python',\n",
        " u'tools',\n",
        " u'ubuntu']"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "np.clip(data, 0, 1, out=data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "array([[0, 0, 0, 1, 1, 0],\n",
        "       [0, 1, 0, 0, 1, 1],\n",
        "       [1, 1, 1, 0, 1, 0]])"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dist = np.sum(data, axis=0)\n",
      "print dist"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1 2 1 1 3 1]\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for tag, count in zip(vocab, dist):\n",
      "    print count, tag"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 distributed systems\n",
        "2 linux\n",
        "1 networking\n",
        "1 python\n",
        "3 tools\n",
        "1 ubuntu\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction import DictVectorizer\n",
      "v = DictVectorizer(sparse=False)\n",
      "D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n",
      "X = v.fit_transform(D)\n",
      "X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "array([[ 2.,  0.,  1.],\n",
        "       [ 0.,  1.,  3.]])"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocab = v.get_feature_names()\n",
      "vocab"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "['bar', 'baz', 'foo']"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v.inverse_transform(X) == \\\n",
      "[{'bar': 2.0, 'foo': 1.0}, {'baz': 1.0, 'foo': 3.0}]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v.transform({'foo': 4, 'unseen_feature': 3})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "array([[ 0.,  0.,  4.]])"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
      "#         Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
      "#         Mathieu Blondel <mathieu@mblondel.org>\n",
      "# License: BSD 3 clause\n",
      "\n",
      "from __future__ import print_function\n",
      "\n",
      "from pprint import pprint\n",
      "from time import time\n",
      "import logging\n",
      "\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "from sklearn.linear_model import SGDClassifier\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "print(__doc__)\n",
      "\n",
      "# Display progress logs on stdout\n",
      "logging.basicConfig(level=logging.INFO,\n",
      "                    format='%(asctime)s %(levelname)s %(message)s')\n",
      "\n",
      "\n",
      "###############################################################################\n",
      "# Load some categories from the training set\n",
      "categories = [\n",
      "    'alt.atheism',\n",
      "    'talk.religion.misc',\n",
      "]\n",
      "# Uncomment the following to do the analysis on all the categories\n",
      "#categories = None\n",
      "\n",
      "print(\"Loading 20 newsgroups dataset for categories:\")\n",
      "print(categories)\n",
      "\n",
      "data = fetch_20newsgroups(subset='train', categories=categories)\n",
      "print(\"%d documents\" % len(data.filenames))\n",
      "print(\"%d categories\" % len(data.target_names))\n",
      "print()\n",
      "\n",
      "###############################################################################\n",
      "# define a pipeline combining a text feature extractor with a simple\n",
      "# classifier\n",
      "pipeline = Pipeline([\n",
      "    ('vect', CountVectorizer()),\n",
      "    ('tfidf', TfidfTransformer()),\n",
      "    ('clf', SGDClassifier()),\n",
      "])\n",
      "\n",
      "# uncommenting more parameters will give better exploring power but will\n",
      "# increase processing time in a combinatorial way\n",
      "parameters = {\n",
      "    'vect__max_df': (0.5, 0.75, 1.0),\n",
      "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
      "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
      "    #'tfidf__use_idf': (True, False),\n",
      "    #'tfidf__norm': ('l1', 'l2'),\n",
      "    'clf__alpha': (0.00001, 0.000001),\n",
      "    'clf__penalty': ('l2', 'elasticnet'),\n",
      "    #'clf__n_iter': (10, 50, 80),\n",
      "}\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # multiprocessing requires the fork to happen in a __main__ protected\n",
      "    # block\n",
      "\n",
      "    # find the best parameters for both the feature extraction and the\n",
      "    # classifier\n",
      "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
      "\n",
      "    print(\"Performing grid search...\")\n",
      "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
      "    print(\"parameters:\")\n",
      "    pprint(parameters)\n",
      "    t0 = time()\n",
      "    grid_search.fit(data.data, data.target)\n",
      "    print(\"done in %0.3fs\" % (time() - t0))\n",
      "    print()\n",
      "\n",
      "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
      "    print(\"Best parameters set:\")\n",
      "    best_parameters = grid_search.best_estimator_.get_params()\n",
      "    for param_name in sorted(parameters.keys()):\n",
      "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}